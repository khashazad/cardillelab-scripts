# and identify lakes with insufficient jrc information
count0 = setnames(temp_f[jrc == 0, .N, by = id], "N", "jrc0")
count1 = setnames(temp_f[jrc == 1, .N, by = id], "N", "jrc1")
count2 = setnames(temp_f[jrc == 2, .N, by = id], "N", "jrc2")
count3 = setnames(temp_f[is.na(jrc), .N, by = id], "N", "jrc3")
count = merge(merge(merge(count0, count1, by = 'id', all = T), count2, by = 'id', all = T), count3, by = 'id', all = T)
count = setnafill(count, fill = 0)
no_jrc = count[(jrc3)/(jrc0 + jrc1 + jrc2 + jrc3) == 1,] # Next identify those lakes that have only jrc 'NA' values (we want to keep those)
no_jrc = no_jrc$id #3185 lakes have no jrc data at all
no_data = count[(jrc0)/(jrc0 + jrc1 + jrc2 + jrc3) > 0.9,]
no_data = no_data$id #1222 lakes have mostly 'no data' values
sporadic = count[(jrc1)/(jrc1 + jrc2) > 0.8,]
sporadic = sporadic[!(id %in% no_data)]
sporadic = sporadic$id #90,414 lakes are sporadic
sp_freeze = temp_f[id %in% sporadic]
sp_freeze1 = setnames(sp_freeze[temp <= 0, .N, by = id], "N", "N_freeze")
sp_freeze2 = sp_freeze[, .N, by = id]
sp_freeze = merge(sp_freeze2, sp_freeze1, all.x = T, by = 'id')
sp_freeze = setnafill(sp_freeze, fill = 0)
sp_freeze = sp_freeze[(N_freeze)/(N) > 0.8,] # But we want to keep sporadic lakes that have > 80% of temp data < 0, because these are permanently frozen.
sp_freeze = sp_freeze$id
sporadic = sporadic[!sporadic %in% sp_freeze] #90,351 lakes are 'sporadic'
intermittent = count[(jrc1)/(jrc1 + jrc2) > 0.2,]
intermittent = intermittent[!(id %in% no_data)]
intermittent = intermittent[!(id %in% sporadic)]
intermittent = intermittent$id #88,443 lakes are 'intermittent'
no_water = count[(jrc1)/(jrc0 + jrc1 + jrc2 + jrc3) == 1,]
no_water = no_water$id #4152 lakes
temp_f = temp_f[!(id %in% sporadic)] # 1,337,270 lakes left
# Remove all 'no water' observations, and 'no data' observations > 40 degrees
temp_f = temp_f[jrc == 0 | jrc == 2 | is.na(jrc)]
temp_f = temp_f[!(jrc == 0 & temp > 40),]
temp_f = temp_f[,c("id", "dateTime", "temp")] # 1,337,270 lakes left
# Apply z-score filtering based on mean
temp_f = temp_f[, mean := mean(temp), by = id]
temp_f = temp_f[, sd := sd(temp), by = id]
temp_f = temp_f[, z_score := abs(temp - mean)/sd, by = id]
temp_f = temp_f[!which(z_score >3 & temp > 0)] # Only filter values with high z-score that are > 0 degrees.
temp_f = temp_f[,c("dateTime", "id", "temp")] # 1,337,270 lakes left
# Filter lakes with <15 data points
less_15 = temp_f[, .N, by = id]
less_15 = less_15[N < 15,]
less_15 = less_15$id
df = temp_f[!(id %in% less_15),] # 1,336,040 lakes left
remove(count, count0, count1, count2, count3, sp_freeze1, sp_freeze2, dup, remove)
gc()
# Run GAMS model
cl = makeCluster(8) # laptop has 6 cores, work has 8
registerDoSNOW(cl)
df_list = foreach(i = 1:8, combine = "rbind", .packages = c("data.table", "mgcv")) %dopar% {
x = gams(df)
return(x)
}
stopCluster(cl)
ls_df = df_list[[1]][[1]]
model_df = df_list[[1]][[2]]
df = merge(model_df, ls_df, by = c('id', 'doy'), all.y = T)
str(ls_df)
str(model_df)
str(df)
model_df = model_df[, id := as.numeric(id)]
df = merge(model_df, ls_df, by = c('id', 'doy'), all.y = T)
remove(df_list, ls_df, model_df)
# Now filter outliers and then repeat model
df = df[, diff := abs(temp - fit)]
df = df[, sd := sd(diff), by = id]
df = df[, z_score := abs(temp - fit)/sd, by = id]
df = df[z_score < 2]
df = df[, c("dateTime","id","temp")]
less_152 = df[, .N, by = id] # Filter lakes with <15 data points
less_152 = less_152[N < 15,]
less_152 = less_152$id
df = df[!(id %in% less_152),] # 1,311,499 lakes with enough data left
less_15 = c(as.character(less_15), less_152)
less_15 = temp_f[id %in% less_15]
remove(less_152)
# Repeat GAMS model
cl = makeCluster(8) # work computer has 8 cores
registerDoSNOW(cl)
df_list = foreach(i = 1:8, combine = "rbind", .packages = c("data.table", "mgcv")) %dopar% {
x = gams(df)
return(x)
}
stopCluster(cl)
ls_df = df_list[[1]][[1]]
model_df = df_list[[1]][[2]]
model_df = model_df[, id := as.numeric(id)]
model_df = model_df[, fit := ifelse(fit < 0, 0, fit)] # We are setting all values < 0 to 0 as we are assuming that this means lake ice
remove(cl, df, df_list, temp_f)
gc()
str(ls_df)
str(model_df)
### Landsat stats from model ----
# Ice duration and start and end days
ice_df = model_df[, .N, by = .(id, fit == 0)]
ice_az = ice_df[fit == FALSE & N == 365,]$id
ice_bz = ice_df[fit == TRUE,]$id
ice_az = model_df[id %in% ice_az]
ice_bz = model_df[id %in% ice_bz]
above_zero = ice_az[, ice := ifelse(fit < 2.5, T, F)]
above_zero = above_zero[, .N, by = .(id, ice == T)]
above_zero = above_zero[ice == TRUE, c(1,3)] %>%
rename("ice_days" = N)
ice_stats = ice_df[fit == TRUE, c(1,3)] %>%
rename("ice_days" = N)
ice_stats = rbind(ice_stats, above_zero)
ice_bz = ice_bz[, cumsum := cumsum(fit), by = id]
ice_bz = ice_bz[fit == 0]
start_bz = ice_bz[cumsum > 0]
start_bz = start_bz[, head(.SD,1), by = id]
setnames(start_bz, "doy", "ice_start")
start_bz = start_bz[, c("id", "ice_start")]
end1 = ice_bz[cumsum == 0]
end1 = end1[, .SD[.N], by = id]
setnames(end1, "doy", "ice_end")
north_lakes = end1$id
end2 = ice_bz[, .SD[.N], by = id]
end2 = end2[!(id %in% north_lakes)]
setnames(end2, "doy", "ice_end")
end_bz = rbind(end1, end2)
end_bz = end_bz[, c("id", "ice_end")]
ice_az = ice_az[fit < 2.5]
start_az1 = ice_az[, head(.SD,1), by = id]
az2 = start_az1[doy == 1,]$id
start_az1 = start_az1[!(id %in% az2)]
az1 = ice_az[!(id %in% az2),]
az2 = ice_az[id %in% az2,]
start_az2 = az2[, delta := doy - shift(doy), by = id]
start_az2 = start_az2[delta > 1,]
start_az = rbind(start_az1[, c("id", "doy")], start_az2[, c("id", "doy")])
setnames(start_az, "doy", "ice_start")
end_az1 = az1[, max(doy), by = id]
setnames(end_az1, "V1", "ice_end")
end_az2 = az2[, delta := doy - shift(doy, n = 1, type = "lead"), by = id]
end_az2 = end_az2[delta < -1, c("id", "doy")]
setnames(end_az2, "doy", "ice_end")
end_az = rbind(end_az1, end_az2)
start = rbind(start_bz, start_az)
end = rbind(end_bz, end_az)
ice_stats = merge(ice_stats, start, all.x = T)
ice_stats = merge(ice_stats, end, all.x = T)
remove(above_zero, az1, az2, end, end_az, end_az1, end_az2, end_bz, end1, end2, ice_az, ice_bz, ice_df, start, start_az, start_az1, start_az2, start_bz, north_lakes)
# Yearly stats
Mean_mod = setnames(model_df[, round(mean(fit),1), by = id], "V1", "Tmean")
Max_mod = setnames(model_df[, round(max(fit),1), by = id], "V1", "Tmax")
Min_mod = setnames(model_df[, round(min(fit),1), by = id], "V1", "Tmin")
stats = Reduce(function(dtf1, dtf2) merge(dtf1, dtf2, by = "id", all.x = TRUE), list(Mean_mod, Max_mod, Min_mod, ice_stats))
stats = stats[, Trange := Tmax - Tmin]
stats[,"ice_days"] = setnafill(stats[,"ice_days"], fill = 0)
stats = stats[,c(1:4,8,5:7)]
remove(Mean_mod, Max_mod, Min_mod, ice_stats)
gc()
str(stats)
# Monthly stats
month = copy(model_df)
month = month[, month := as.numeric(format(as.Date(paste(2001, doy), format = "%Y %j"), "%m"))]
month = setnames(month[, round(mean(fit),1), by = c("id", "month")], "V1", "Tmonth_mean")
ls_df = ls_df[, month := as.numeric(format(as.Date(paste(2001, doy), format = "%Y %j"), "%m"))]
CI = merge(ls_df, month, by = c('id', 'month'), all = T)
CI = CI[, temp := ifelse(temp < 0, 0, temp)]
CI = CI[, diff := abs(temp - Tmonth_mean)]
CI = CI[, sd := sd(diff), by = c('id', 'month')]
CI = CI[, df := (.N-1), by = c('id', 'month')]
CI = CI[, Tmonth_CImax := ifelse(df > 30, Tmonth_mean + (sd * 2),
Tmonth_mean + (sd * qt(0.025, df = df, lower.tail = F)))]
CI = CI[, Tmonth_CImin := ifelse(df > 30, Tmonth_mean - (sd * 2),
Tmonth_mean - (sd * qt(0.025, df = df, lower.tail = F)))]
CI = CI[, Tmonth_CImax := ifelse(Tmonth_mean == 0, NA, Tmonth_CImax)]
CI = CI[, Tmonth_CImin := ifelse(Tmonth_mean == 0, NA, Tmonth_CImin)]
CI = CI[, Tmonth_CImin := ifelse(Tmonth_CImin < 0, 0, Tmonth_CImin)]
CI = CI[, Tmonth_CImax := ifelse(Tmonth_CImax == 'NaN', NA,Tmonth_CImax)]
CI = CI[, Tmonth_CImin := ifelse(Tmonth_CImin == 'NaN', NA,Tmonth_CImin)]
CI = CI[, c(1,2,10,11)]
CI = CI[, head(.SD,1), by = c('id', "month")]
CI1 = CI[, c(1:3)]
CI1 = pivot_wider(CI1, names_from = 'month', names_prefix = 'Tmonth_CImax_', values_from = 'Tmonth_CImax')
CI2 = CI[, c(1,2,4)]
CI2 = pivot_wider(CI2, names_from = 'month', names_prefix = 'Tmonth_CImin_', values_from = 'Tmonth_CImin')
CI = data.table(merge(CI1, CI2, by = 'id'))
cols = names(CI[,-"id"])
CI = CI[, (cols) := round(.SD,1), .SDcols=cols]
month = data.table(pivot_wider(month, names_from = "month", values_from = "Tmonth_mean", names_prefix = 'Tmonth_mean_'))
month = merge(month, CI, by = 'id')
str(month)
stats = setnames(merge(stats, month, by = 'id'), 'id', 'Hylak_id')
str(stats)
remove(CI1, CI2, month, cols, model_df, CI)
str(less_15)
### Stats for lakes with <15 datapoints ----
less_copy = copy(less_15)
less_copy = less_copy[, temp := ifelse(temp < 0, 0, temp)]
Tmean = setnames(less_copy[, mean(temp), by = 'id'], c("id", "V1"), c("Hylak_id", "Tmean"))
Tmax = setnames(less_copy[, max(temp), by = 'id'], c("id", "V1"), c("Hylak_id", "Tmax"))
Tmin = setnames(less_copy[, min(temp), by = 'id'], c("id", "V1"), c("Hylak_id", "Tmin"))
stats_15 = merge(merge(Tmean, Tmax, by = 'Hylak_id'), Tmin, by = 'Hylak_id')
stats_15 = stats_15[, Trange := Tmax - Tmin]
cols = names(stats_15[,-"Hylak_id"])
stats_15 = stats_15[,(cols) := round(.SD,1), .SDcols=cols]
cols = names(stats[,-c("Tmean", "Tmax", "Tmin", "Trange", "Hylak_id")])
stats_15 = stats_15[, c(cols) := NA]
cols = names(stats)
stats_15 = setcolorder(stats_15, cols)
stats = rbind(stats, stats_15)
less_15_id = unique(less_15$id)
remove(stats_15, cols, Tmax, Tmin, Tmean, less_copy)
# Flags
stats = stats[, stat_method := ifelse(Hylak_id %in% less_15_id, "B", "A")]
stats = stats[, intermittent := ifelse(Hylak_id %in% intermittent, "Y", "N")]
# Add count to stats
summary1 = setnames(ls_df[, .N, by = 'id'], c('id', 'N'), c('Hylak_id', 'n_obs'))
summary2 = setnames(less_15[, .N, by = 'id'], c('id', 'N'), c('Hylak_id', 'n_obs'))
summary = rbind(summary1, summary2)
str(summary)
test = merge(stats, summary, by = 'Hylak_id', all = T)
View(test)
remove(summary1, summary2, summary)
View(less_15)
### Add lat/long info to stats file ----
setwd("C:/Users/Maartje/OneDrive - McGill University/PhD research/Chapter1/Analysis/Data/Lake_centers_by_fishnet/Fishnet1")
files = list.files(pattern="*.csv")
fish1 = rbindlist(lapply(files, fread))
fish1 = fish1[,-4]
setwd("C:/Users/Maartje/OneDrive - McGill University/PhD research/Chapter1/Analysis/Data/Lake_centers_by_fishnet/Fishnet2")
files = list.files(pattern="*.csv")
fish2 = rbindlist(lapply(files, fread))
fish2 = fish2[,-4]
setwd("C:/Users/Maartje/OneDrive - McGill University/PhD research/Chapter1/Analysis/Data/Lake_centers_by_fishnet/Fishnet3")
files = list.files(pattern="*.csv")
fish3 = rbindlist(lapply(files, fread))
fish3 = fish3[,-4]
fish = rbindlist(list(fish1,fish2,fish3))
remove(fish1, fish2, fish3)
stats = merge(stats, fish, by = 'Hylak_id', all.x = T)
View(stats)
stats = stats[,c(1,47:48,45:46,2:44)]
View(stats)
View(ls_df)
ls_df = ls_df[, c("dateTime", "id", "temp")]
df = rbind(ls_df, less_15)
# This script processes the entire dataset
library(data.table)
library(lubridate)
library(ggplot2)
library(outliers)
library(mgcv)
library(scales)
library(stats)
library(tidyverse)
library(foreach)
library(doSNOW)
library(parallel)
source("C:/Users/Maartje/OneDrive - McGill University/PhD research/Chapter1/Analysis/Scripts/gams.R")
### Load landsat data that was merged with jrc in 'load_lsdata.R' ----
temp = fread("C:/Users/Maartje/OneDrive - McGill University/PhD research/Chapter1/Analysis/Data/Temp_data/Temp_jrc_merge/temp_jrc.csv") #1,427,621 lakes. 1,427,688 lakes in HydroLAKES
setnames(temp, "Hylak_id", "id")
hylaks = unique(temp$id)
hylaks = hylaks[1:5000]
temp = temp[id %in% hylaks]
gc()
### Filter dataset ----
temp_f = temp[dateTime < "2021-01-01"] #1,427,621 lakes.
remove(temp)
gc()
length(unique(temp_f$id))
# Replace duplicate readings (same id and hour) with one average value and remove those > 0.2 degrees different
temp_f = temp_f[, hour := floor_date(dateTime, "hour")]
temp_f = temp_f[, dup1 := duplicated(hour, fromLast = T), by = id]
temp_f = temp_f[, dup2 := duplicated(hour, fromLast = F), by = id]
dup = copy(temp_f)
dup = dup[dup1 == TRUE | dup2 == TRUE]
dup = dup[, c("id", "temp", "hour", "dup1")]
dup = data.table(pivot_wider(dup, names_from = "dup1", values_from = "temp", id_cols = c("id", "hour")))
setnames(dup, c("TRUE", "FALSE"), c("temp1", "temp2"))
dup = dup[, diff := temp1 - temp2]
dup = dup[, temp_r := (temp1 + temp2)/2]
remove = dup[diff < -0.2 | diff > 0.2, c("id", "hour")] # However we remove the readings for which the duplicates differ more than 0.2 degrees
remove = remove[, remove := TRUE]
dup = dup[, c("id", "hour", "temp_r")]
temp_f = temp_f[dup1 == FALSE, -c("dup1", "dup2")]
temp_f = merge(temp_f, remove, by = c("id", "hour"), all.x = T)
temp_f = temp_f[is.na(remove),]
temp_f = merge(temp_f, dup, by = c("id", "hour"), all.x = T)
temp_f = temp_f[, temp := ifelse(is.na(temp_r), temp, temp_r)]
temp_f = temp_f[, c("id", "dateTime", "temp", "jrc")] # 1,427,621 lakes.
gc()
# Remove sporadic lakes (< 20% of their observations being 'water')
# and identify intermittent lakes(< 80% of their observations being 'water')
# and identify lakes with insufficient jrc information
count0 = setnames(temp_f[jrc == 0, .N, by = id], "N", "jrc0")
count1 = setnames(temp_f[jrc == 1, .N, by = id], "N", "jrc1")
count2 = setnames(temp_f[jrc == 2, .N, by = id], "N", "jrc2")
count3 = setnames(temp_f[is.na(jrc), .N, by = id], "N", "jrc3")
count = merge(merge(merge(count0, count1, by = 'id', all = T), count2, by = 'id', all = T), count3, by = 'id', all = T)
count = setnafill(count, fill = 0)
no_jrc = count[(jrc3)/(jrc0 + jrc1 + jrc2 + jrc3) == 1,] # Next identify those lakes that have only jrc 'NA' values (we want to keep those)
no_jrc = no_jrc$id #3185 lakes have no jrc data at all
no_data = count[(jrc0)/(jrc0 + jrc1 + jrc2 + jrc3) > 0.9,]
no_data = no_data$id #1222 lakes have mostly 'no data' values
sporadic = count[(jrc1)/(jrc1 + jrc2) > 0.8,]
sporadic = sporadic[!(id %in% no_data)]
sporadic = sporadic$id #90,414 lakes are sporadic
sp_freeze = temp_f[id %in% sporadic]
sp_freeze1 = setnames(sp_freeze[temp <= 0, .N, by = id], "N", "N_freeze")
sp_freeze2 = sp_freeze[, .N, by = id]
sp_freeze = merge(sp_freeze2, sp_freeze1, all.x = T, by = 'id')
sp_freeze = setnafill(sp_freeze, fill = 0)
sp_freeze = sp_freeze[(N_freeze)/(N) > 0.8,] # But we want to keep sporadic lakes that have > 80% of temp data < 0, because these are permanently frozen.
sp_freeze = sp_freeze$id
sporadic = sporadic[!sporadic %in% sp_freeze] #90,351 lakes are 'sporadic'
intermittent = count[(jrc1)/(jrc1 + jrc2) > 0.2,]
intermittent = intermittent[!(id %in% no_data)]
intermittent = intermittent[!(id %in% sporadic)]
intermittent = intermittent$id #88,443 lakes are 'intermittent'
no_water = count[(jrc1)/(jrc0 + jrc1 + jrc2 + jrc3) == 1,]
no_water = no_water$id #4152 lakes
temp_f = temp_f[!(id %in% sporadic)] # 1,337,270 lakes left
# Remove all 'no water' observations, and 'no data' observations > 40 degrees
temp_f = temp_f[jrc == 0 | jrc == 2 | is.na(jrc)]
temp_f = temp_f[!(jrc == 0 & temp > 40),]
temp_f = temp_f[,c("id", "dateTime", "temp")] # 1,337,270 lakes left
# Apply z-score filtering based on mean
temp_f = temp_f[, mean := mean(temp), by = id]
temp_f = temp_f[, sd := sd(temp), by = id]
temp_f = temp_f[, z_score := abs(temp - mean)/sd, by = id]
temp_f = temp_f[!which(z_score >3 & temp > 0)] # Only filter values with high z-score that are > 0 degrees.
temp_f = temp_f[,c("dateTime", "id", "temp")] # 1,337,270 lakes left
# Filter lakes with <15 data points
less_15 = temp_f[, .N, by = id]
less_15 = less_15[N < 15,]
less_15 = less_15$id
df = temp_f[!(id %in% less_15),] # 1,336,040 lakes left
remove(count, count0, count1, count2, count3, sp_freeze1, sp_freeze2, dup, remove)
gc()
# Run GAMS model
cl = makeCluster(8) # laptop has 6 cores, work has 8
registerDoSNOW(cl)
df_list = foreach(i = 1:8, combine = "rbind", .packages = c("data.table", "mgcv")) %dopar% {
x = gams(df)
return(x)
}
stopCluster(cl)
ls_df = df_list[[1]][[1]]
model_df = df_list[[1]][[2]]
model_df = model_df[, id := as.numeric(id)]
df = merge(model_df, ls_df, by = c('id', 'doy'), all.y = T)
remove(df_list, ls_df, model_df)
# Now filter outliers and then repeat model
df = df[, diff := abs(temp - fit)]
df = df[, sd := sd(diff), by = id]
df = df[, z_score := abs(temp - fit)/sd, by = id]
df = df[z_score < 2]
df = df[, c("dateTime","id","temp")]
less_152 = df[, .N, by = id] # Filter lakes with <15 data points
less_152 = less_152[N < 15,]
less_152 = less_152$id
df = df[!(id %in% less_152),] # 1,311,499 lakes with enough data left
less_15 = c(as.character(less_15), less_152)
less_15 = temp_f[id %in% less_15]
remove(less_152)
# Repeat GAMS model
cl = makeCluster(8) # work computer has 8 cores
registerDoSNOW(cl)
df_list = foreach(i = 1:8, combine = "rbind", .packages = c("data.table", "mgcv")) %dopar% {
x = gams(df)
return(x)
}
stopCluster(cl)
ls_df = df_list[[1]][[1]]
model_df = df_list[[1]][[2]]
model_df = model_df[, id := as.numeric(id)]
model_df = model_df[, fit := ifelse(fit < 0, 0, fit)] # We are setting all values < 0 to 0 as we are assuming that this means lake ice
remove(cl, df, df_list, temp_f)
gc()
### Landsat stats from model ----
# Ice duration and start and end days
ice_df = model_df[, .N, by = .(id, fit == 0)]
ice_az = ice_df[fit == FALSE & N == 365,]$id
ice_bz = ice_df[fit == TRUE,]$id
ice_az = model_df[id %in% ice_az]
ice_bz = model_df[id %in% ice_bz]
above_zero = ice_az[, ice := ifelse(fit < 2.5, T, F)]
above_zero = above_zero[, .N, by = .(id, ice == T)]
above_zero = above_zero[ice == TRUE, c(1,3)] %>%
rename("ice_days" = N)
ice_stats = ice_df[fit == TRUE, c(1,3)] %>%
rename("ice_days" = N)
ice_stats = rbind(ice_stats, above_zero)
ice_bz = ice_bz[, cumsum := cumsum(fit), by = id]
ice_bz = ice_bz[fit == 0]
start_bz = ice_bz[cumsum > 0]
start_bz = start_bz[, head(.SD,1), by = id]
setnames(start_bz, "doy", "ice_start")
start_bz = start_bz[, c("id", "ice_start")]
end1 = ice_bz[cumsum == 0]
end1 = end1[, .SD[.N], by = id]
setnames(end1, "doy", "ice_end")
north_lakes = end1$id
end2 = ice_bz[, .SD[.N], by = id]
end2 = end2[!(id %in% north_lakes)]
setnames(end2, "doy", "ice_end")
end_bz = rbind(end1, end2)
end_bz = end_bz[, c("id", "ice_end")]
ice_az = ice_az[fit < 2.5]
start_az1 = ice_az[, head(.SD,1), by = id]
az2 = start_az1[doy == 1,]$id
start_az1 = start_az1[!(id %in% az2)]
az1 = ice_az[!(id %in% az2),]
az2 = ice_az[id %in% az2,]
start_az2 = az2[, delta := doy - shift(doy), by = id]
start_az2 = start_az2[delta > 1,]
start_az = rbind(start_az1[, c("id", "doy")], start_az2[, c("id", "doy")])
setnames(start_az, "doy", "ice_start")
end_az1 = az1[, max(doy), by = id]
setnames(end_az1, "V1", "ice_end")
end_az2 = az2[, delta := doy - shift(doy, n = 1, type = "lead"), by = id]
end_az2 = end_az2[delta < -1, c("id", "doy")]
setnames(end_az2, "doy", "ice_end")
end_az = rbind(end_az1, end_az2)
start = rbind(start_bz, start_az)
end = rbind(end_bz, end_az)
ice_stats = merge(ice_stats, start, all.x = T)
ice_stats = merge(ice_stats, end, all.x = T)
remove(above_zero, az1, az2, end, end_az, end_az1, end_az2, end_bz, end1, end2, ice_az, ice_bz, ice_df, start, start_az, start_az1, start_az2, start_bz, north_lakes)
# Yearly stats
Mean_mod = setnames(model_df[, round(mean(fit),1), by = id], "V1", "Tmean")
Max_mod = setnames(model_df[, round(max(fit),1), by = id], "V1", "Tmax")
Min_mod = setnames(model_df[, round(min(fit),1), by = id], "V1", "Tmin")
stats = Reduce(function(dtf1, dtf2) merge(dtf1, dtf2, by = "id", all.x = TRUE), list(Mean_mod, Max_mod, Min_mod, ice_stats))
stats = stats[, Trange := Tmax - Tmin]
stats[,"ice_days"] = setnafill(stats[,"ice_days"], fill = 0)
stats = stats[,c(1:4,8,5:7)]
remove(Mean_mod, Max_mod, Min_mod, ice_stats)
gc()
# Monthly stats
month = copy(model_df)
month = month[, month := as.numeric(format(as.Date(paste(2001, doy), format = "%Y %j"), "%m"))]
month = setnames(month[, round(mean(fit),1), by = c("id", "month")], "V1", "Tmonth_mean")
ls_df = ls_df[, month := as.numeric(format(as.Date(paste(2001, doy), format = "%Y %j"), "%m"))]
CI = merge(ls_df, month, by = c('id', 'month'), all = T)
CI = CI[, temp := ifelse(temp < 0, 0, temp)]
CI = CI[, diff := abs(temp - Tmonth_mean)]
CI = CI[, sd := sd(diff), by = c('id', 'month')]
CI = CI[, df := (.N-1), by = c('id', 'month')]
CI = CI[, Tmonth_CImax := ifelse(df > 30, Tmonth_mean + (sd * 2),
Tmonth_mean + (sd * qt(0.025, df = df, lower.tail = F)))]
CI = CI[, Tmonth_CImin := ifelse(df > 30, Tmonth_mean - (sd * 2),
Tmonth_mean - (sd * qt(0.025, df = df, lower.tail = F)))]
CI = CI[, Tmonth_CImax := ifelse(Tmonth_mean == 0, NA, Tmonth_CImax)]
CI = CI[, Tmonth_CImin := ifelse(Tmonth_mean == 0, NA, Tmonth_CImin)]
CI = CI[, Tmonth_CImin := ifelse(Tmonth_CImin < 0, 0, Tmonth_CImin)]
CI = CI[, Tmonth_CImax := ifelse(Tmonth_CImax == 'NaN', NA,Tmonth_CImax)]
CI = CI[, Tmonth_CImin := ifelse(Tmonth_CImin == 'NaN', NA,Tmonth_CImin)]
CI = CI[, c(1,2,10,11)]
CI = CI[, head(.SD,1), by = c('id', "month")]
CI1 = CI[, c(1:3)]
CI1 = pivot_wider(CI1, names_from = 'month', names_prefix = 'Tmonth_CImax_', values_from = 'Tmonth_CImax')
CI2 = CI[, c(1,2,4)]
CI2 = pivot_wider(CI2, names_from = 'month', names_prefix = 'Tmonth_CImin_', values_from = 'Tmonth_CImin')
CI = data.table(merge(CI1, CI2, by = 'id'))
cols = names(CI[,-"id"])
CI = CI[, (cols) := round(.SD,1), .SDcols=cols]
month = data.table(pivot_wider(month, names_from = "month", values_from = "Tmonth_mean", names_prefix = 'Tmonth_mean_'))
month = merge(month, CI, by = 'id')
stats = setnames(merge(stats, month, by = 'id'), 'id', 'Hylak_id')
remove(CI1, CI2, month, cols, model_df, CI)
### Stats for lakes with <15 datapoints ----
less_copy = copy(less_15)
less_copy = less_copy[, temp := ifelse(temp < 0, 0, temp)]
Tmean = setnames(less_copy[, mean(temp), by = 'id'], c("id", "V1"), c("Hylak_id", "Tmean"))
Tmax = setnames(less_copy[, max(temp), by = 'id'], c("id", "V1"), c("Hylak_id", "Tmax"))
Tmin = setnames(less_copy[, min(temp), by = 'id'], c("id", "V1"), c("Hylak_id", "Tmin"))
stats_15 = merge(merge(Tmean, Tmax, by = 'Hylak_id'), Tmin, by = 'Hylak_id')
stats_15 = stats_15[, Trange := Tmax - Tmin]
cols = names(stats_15[,-"Hylak_id"])
stats_15 = stats_15[,(cols) := round(.SD,1), .SDcols=cols]
cols = names(stats[,-c("Tmean", "Tmax", "Tmin", "Trange", "Hylak_id")])
stats_15 = stats_15[, c(cols) := NA]
cols = names(stats)
stats_15 = setcolorder(stats_15, cols)
stats = rbind(stats, stats_15)
less_15_id = unique(less_15$id)
remove(stats_15, cols, Tmax, Tmin, Tmean, less_copy)
# Flags
stats = stats[, stat_method := ifelse(Hylak_id %in% less_15_id, "B", "A")]
stats = stats[, intermittent := ifelse(Hylak_id %in% intermittent, "Y", "N")]
# Add count to stats
summary1 = setnames(ls_df[, .N, by = 'id'], c('id', 'N'), c('Hylak_id', 'n_obs'))
summary2 = setnames(less_15[, .N, by = 'id'], c('id', 'N'), c('Hylak_id', 'n_obs'))
summary = rbind(summary1, summary2)
stats = merge(stats, summary, by = 'Hylak_id', all = T)
remove(summary1, summary2, summary)
### Add lat/long info to stats file ----
setwd("C:/Users/Maartje/OneDrive - McGill University/PhD research/Chapter1/Analysis/Data/Lake_centers_by_fishnet/Fishnet1")
files = list.files(pattern="*.csv")
fish1 = rbindlist(lapply(files, fread))
fish1 = fish1[,-4]
setwd("C:/Users/Maartje/OneDrive - McGill University/PhD research/Chapter1/Analysis/Data/Lake_centers_by_fishnet/Fishnet2")
files = list.files(pattern="*.csv")
fish2 = rbindlist(lapply(files, fread))
fish2 = fish2[,-4]
setwd("C:/Users/Maartje/OneDrive - McGill University/PhD research/Chapter1/Analysis/Data/Lake_centers_by_fishnet/Fishnet3")
files = list.files(pattern="*.csv")
fish3 = rbindlist(lapply(files, fread))
fish3 = fish3[,-4]
fish = rbindlist(list(fish1,fish2,fish3))
remove(fish1, fish2, fish3)
stats = merge(stats, fish, by = 'Hylak_id', all.x = T)
View(stats)
stats = stats[,c(1,48:49,47,45:46,2:44)]
View(stats)
ls_df = ls_df[, c("dateTime", "id", "temp")]
df = rbind(ls_df, less_15)
